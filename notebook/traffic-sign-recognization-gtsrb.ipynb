{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T02:54:37.275097684Z",
     "start_time": "2023-12-01T02:54:36.989273716Z"
    }
   },
   "outputs": [],
   "source": [
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any,Tuple,Optional,Callable\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T02:55:35.457100388Z",
     "start_time": "2023-12-01T02:55:35.456693491Z"
    }
   },
   "outputs": [],
   "source": [
    "class GTSRB(Dataset):\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 split: str,\n",
    "                 transform: Optional[Callable] = None):\n",
    "        \n",
    "        self.base_folder = pathlib.Path(root)\n",
    "        self.csv_file = self.base_folder / ('Train.csv' if split =='train' else 'Test.csv')\n",
    "        \n",
    "        with open('/'+str(self.csv_file)) as csvfile:\n",
    "           samples = [('/'+str(self.base_folder / row['Path']),int(row['ClassId'])) \n",
    "            for row in csv.DictReader(csvfile,delimiter=',',skipinitialspace=True)\n",
    "                ]\n",
    "\n",
    "        self.samples = samples\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        path,classId =  self.samples[index]\n",
    "        sample = PIL.Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample,classId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BrP9IW5hyZ0"
   },
   "source": [
    "### Read data from datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UmO3XzHR295",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.476040331Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/amber/Documents/sign/gtsrb-german-traffic-sign'\n",
    "\n",
    "train_data = GTSRB(root=data_dir,split=\"train\")\n",
    "test_data = GTSRB(root=data_dir,split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T02:55:35.482435856Z",
     "start_time": "2023-12-01T02:55:35.476176373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label Overview\n",
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.476307852Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = '/home/amber/Documents/sign/gtsrb-german-traffic-sign/Train'\n",
    "\n",
    "folders = os.listdir(train_path)\n",
    "\n",
    "train_number = []\n",
    "class_num = []\n",
    "\n",
    "for folder in folders:\n",
    "    train_files = os.listdir(train_path + '/' + folder)\n",
    "    train_number.append(len(train_files))\n",
    "    class_num.append(classes[int(folder)])\n",
    "    \n",
    "# Sorting the dataset on the basis of number of images in each class\n",
    "zipped_lists = zip(train_number, class_num)\n",
    "sorted_pairs = sorted(zipped_lists)\n",
    "tuples = zip(*sorted_pairs)\n",
    "train_number, class_num = [ list(tuple) for tuple in  tuples]\n",
    "\n",
    "# Plotting the number of images in each class\n",
    "plt.figure(figsize=(21,10))  \n",
    "plt.bar(class_num, train_number)\n",
    "plt.xticks(class_num, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.476396847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing 25 random images from test data\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "data_dir = '/home/amber/Documents/sign/gtsrb-german-traffic-sign'\n",
    "\n",
    "test = pd.read_csv(data_dir + '/Test.csv')\n",
    "imgs = test[\"Path\"].values\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "for i in range(1,26):\n",
    "    plt.subplot(5,5,i)\n",
    "    random_img_path = data_dir + '/' + random.choice(imgs)\n",
    "    rand_img = imread(random_img_path)\n",
    "    plt.imshow(rand_img)\n",
    "    plt.grid()\n",
    "    plt.xlabel(rand_img.shape[1], fontsize = 20)#width of image\n",
    "    plt.ylabel(rand_img.shape[0], fontsize = 20)#height of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.476490880Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "#     v2.ToPILImage(),\n",
    "\n",
    "    v2.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "    v2.RandomEqualize(0.4),\n",
    "    v2.AugMix(),\n",
    "    v2.RandomHorizontalFlip(p=0.3),\n",
    "    v2.RandomVerticalFlip(0.3),\n",
    "    v2.GaussianBlur((3,3)),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.Resize(size=(50,50)),\n",
    "    v2.PILToTensor(),\n",
    "\n",
    "])\n",
    "validation_transforms =  v2.Compose([\n",
    "    v2.Resize(size=(50,50)),\n",
    "    v2.PILToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.476646867Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(dataset,train_size):\n",
    "\n",
    "    train_size = int(train_size * len(dataset))\n",
    "    test_size = int(len(dataset) - train_size)\n",
    "    return random_split(dataset,[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477084485Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = GTSRB(root='/home/amber/Documents/sign/gtsrb-german-traffic-sign',split=\"train\")\n",
    "train_set,validation_set = train_test_split(train_dataset,train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477361367Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set.dataset.transform = train_transforms\n",
    "validation_set.dataset.transform = validation_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477470119Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_set,batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLjFvNjDFIAi"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MV5YeepgJch-",
    "outputId": "02882c0b-ce5f-4432-a4c6-45ef874afb8e",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477540246Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQt7skizSYHo",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477591998Z"
    }
   },
   "outputs": [],
   "source": [
    "class GTSRB_MODEL(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(GTSRB_MODEL,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "      \n",
    "        self.metrics = {}\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "       \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3)\n",
    "        self.conv6 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "       \n",
    "       \n",
    "\n",
    "        self.l1 = nn.Linear(1024*4*4,512)\n",
    "        self.l2 = nn.Linear(512,128)\n",
    "        self.batchnorm4 = nn.LayerNorm(128)\n",
    "        self.l3 = nn.Linear(128,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        conv = self.conv1(input)\n",
    "        conv = self.conv2(conv)\n",
    "        batchnorm = self.relu(self.batchnorm1(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv3(maxpool)\n",
    "        conv = self.conv4(conv)\n",
    "        batchnorm = self.relu(self.batchnorm2(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv5(maxpool)\n",
    "        conv = self.conv6(conv)\n",
    "        batchnorm = self.relu(self.batchnorm3(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "        \n",
    "        \n",
    "        flatten = self.flatten(maxpool)\n",
    "        \n",
    "        dense_l1 = self.l1(flatten)\n",
    "        dropout = self.dropout3(dense_l1)\n",
    "        dense_l2 = self.l2(dropout)\n",
    "        batchnorm = self.batchnorm4(dense_l2)\n",
    "        dropout = self.dropout2(batchnorm)\n",
    "        output = self.l3(dropout)\n",
    "        \n",
    "       \n",
    "        return output\n",
    "    \n",
    "    def training_metrics(self,positives,data_size,loss):\n",
    "        acc = positives/data_size\n",
    "        return loss,acc\n",
    "    \n",
    "    def validation_metrics(self,validation_data,loss_function):\n",
    "       data_size = len(validation_data)\n",
    "       correct_predictions = 0\n",
    "       total_samples = 0\n",
    "       val_loss = 0\n",
    "\n",
    "       model = self.eval()\n",
    "       with torch.no_grad() : \n",
    "        for step,(input,label) in enumerate(validation_data):\n",
    "            input = input.to(torch.float)\n",
    "\n",
    "            input,label = input.to(device),label.to(device)\n",
    "            prediction = model.forward(input)\n",
    "            loss = loss_function(prediction,label)\n",
    "            val_loss = loss.item()\n",
    "            _,predicted = torch.max(prediction,1)\n",
    "            correct_predictions += (predicted == label).sum().item()\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "       val_acc = correct_predictions/total_samples\n",
    "\n",
    "       return val_loss,val_acc\n",
    "\n",
    "    def history(self):\n",
    "        return self.metrics\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def compile(self,train_data,validation_data,epochs,loss_function,optimizer,learning_rate_scheduler):\n",
    "        val_acc_list = []\n",
    "        val_loss_list = []\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_loss_list = []\n",
    "\n",
    "        learning_rate_list = []\n",
    "\n",
    "        print('training started ...')\n",
    "        STEPS = len(train_data)\n",
    "        for epoch in range(epochs):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            learning_rate_list.append(lr)\n",
    "            correct_predictions = 0\n",
    "            total_examples = 0\n",
    "            loss = 0\n",
    "            with tqdm.trange(STEPS) as progress:\n",
    "\n",
    "                for step,(input,label) in enumerate(train_loader):\n",
    "                    input = input.to(torch.float)\n",
    "#                     input.type()\n",
    "\n",
    "\n",
    "                    input,label = input.to(device),label.to(device)\n",
    "                    prediction = self.forward(input)\n",
    "\n",
    "                    _, predicted = torch.max(prediction, 1)\n",
    "                    correct_predictions += (predicted == label).sum().item()\n",
    "                    total_examples += label.size(0)\n",
    "                    l = loss_function(prediction,label)\n",
    "                    loss = l.item()\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    progress.colour = 'green'\n",
    "                    progress.desc = f'Epoch [{epoch}/{EPOCHS}], Step [{step}/{STEPS}], Learning Rate [{lr}], Loss [{\"{:.4f}\".format(l)}], Accuracy [{\"{:.4f}\".format(correct_predictions/total_examples)}]'\n",
    "                    progress.update(1)\n",
    "\n",
    "            training_loss,training_acc = self.training_metrics(correct_predictions,total_examples,loss)\n",
    "            train_acc_list.append(training_acc)\n",
    "            train_loss_list.append(training_loss)\n",
    "\n",
    "            val_loss, val_acc = self.validation_metrics(validation_data,loss_function)\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            print(f'val_accuracy [{val_acc}], val_loss [{val_loss}]')\n",
    "\n",
    "            \n",
    "            learning_rate_scheduler.step()\n",
    "        \n",
    "        metrics_dict = {\n",
    "                'train_acc':train_acc_list,\n",
    "                'train_loss':train_loss_list,\n",
    "                'val_acc':val_acc_list,\n",
    "                'val_loss':val_loss_list,\n",
    "                'learning_rate':optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "        self.metrics = metrics_dict\n",
    "        print('training complete !')    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqCPI1ZhSxa5",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477648189Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0008\n",
    "INPUT_DIM = 3*50*50\n",
    "OUTPUT_DIM = 43\n",
    "model = GTSRB_MODEL(INPUT_DIM,OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(),lr=LEARNING_RATE)\n",
    "lr_s = lr_scheduler.LinearLR(optimizer,start_factor=1.0,end_factor=0.5,total_iters=10)\n",
    "loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2U_RLsFVYCC",
    "outputId": "f36d0b32-8782-49f5-b33f-ddb37e86be51",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477699504Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(train_data=train_loader,\n",
    "              validation_data=validation_loader,\n",
    "              epochs=EPOCHS,\n",
    "              loss_function=loss,\n",
    "              optimizer=optimizer,\n",
    "              learning_rate_scheduler=lr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T22:13:37.972041Z",
     "iopub.status.busy": "2023-11-23T22:13:37.971705Z",
     "iopub.status.idle": "2023-11-23T22:13:38.256394Z",
     "shell.execute_reply": "2023-11-23T22:13:38.255106Z",
     "shell.execute_reply.started": "2023-11-23T22:13:37.972011Z"
    },
    "id": "KBRfcAdsUSvA",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477751031Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving a checkpoint assuming the network class named ClassNet\n",
    "checkpoint = {'model': model,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'epoch': EPOCHS,\n",
    "              'loss': loss,}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:18:34.630378Z",
     "iopub.status.busy": "2023-11-22T05:18:34.629376Z",
     "iopub.status.idle": "2023-11-22T05:18:34.873541Z",
     "shell.execute_reply": "2023-11-22T05:18:34.872435Z",
     "shell.execute_reply.started": "2023-11-22T05:18:34.630323Z"
    },
    "id": "F7ahee-LU68o",
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477801938Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Loading model\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "\n",
    "    model = checkpoint['model']  # 提取网络结构\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # 加载网络权重参数\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # 加载优化器参数\n",
    "    # epoch = checkpoint['epoch']\n",
    "    # loss = checkpoint['loss']\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "trained_model = load_checkpoint('/home/amber/Documents/sign/model/checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.477853989Z"
    }
   },
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.Resize(size=(50,50)),\n",
    "    v2.ToImageTensor(),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.478008157Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = GTSRB(root='/home/amber/Documents/sign/gtsrb-german-traffic-sign',split='test',transform=transforms)\n",
    "test_dataloader = DataLoader(dataset = test_dataset)\n",
    "print(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T05:18:34.876593Z",
     "iopub.status.busy": "2023-11-22T05:18:34.875274Z",
     "iopub.status.idle": "2023-11-22T05:18:37.071973Z",
     "shell.execute_reply": "2023-11-22T05:18:37.068200Z",
     "shell.execute_reply.started": "2023-11-22T05:18:34.876547Z"
    },
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.478065545Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "trained_model = trained_model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input = input.to(torch.float)\n",
    "        input,label = input.to(device),label\n",
    "        y_true = label.cpu().detach().numpy()\n",
    "        \n",
    "        prediction = trained_model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T02:55:35.478145812Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 82373,
     "sourceId": 191501,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
