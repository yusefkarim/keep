{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import opendatasets as od"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:18:56.411459676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./gtsrb-german-traffic-sign\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:18:56.805222515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple,Optional,Callable\n",
    "\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:18:56.805412303Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Device-GPU/CPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:19:02.368155531Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GTSRB(Dataset):\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 split: str,\n",
    "                 transform: Optional[Callable] = None):\n",
    "        \n",
    "        self.base_folder = pathlib.Path(root)\n",
    "        self.csv_file = self.base_folder / ('Train.csv' if split =='train' else 'Test.csv')\n",
    "        \n",
    "        with open('/'+str(self.csv_file)) as csvfile:\n",
    "           samples = [('/'+str(self.base_folder / row['Path']),int(row['ClassId'])) \n",
    "            for row in csv.DictReader(csvfile,delimiter=',',skipinitialspace=True)\n",
    "                ]\n",
    "\n",
    "        self.samples = samples\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        path,classId =  self.samples[index]\n",
    "        sample = PIL.Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample,classId"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:18:56.858972145Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data from datalake"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_dir = '/home/amber/Documents/sign/gtsrb-german-traffic-sign'\n",
    "\n",
    "train_data = GTSRB(root=data_dir,split=\"train\")\n",
    "test_data = GTSRB(root=data_dir,split=\"test\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:18:56.962397187Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split train/test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def train_test_split(dataset,train_size):\n",
    "\n",
    "    train_size = int(train_size * len(dataset))\n",
    "    test_size = int(len(dataset) - train_size)\n",
    "    return random_split(dataset,[train_size,test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:19:01.604922859Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split train/validation set from train dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "train_dataset = GTSRB(root='/home/amber/Documents/sign/gtsrb-german-traffic-sign',split=\"train\")\n",
    "train_set,validation_set = train_test_split(train_dataset,train_size=0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:19:01.742703755Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "#     v2.ToPILImage(),\n",
    "\n",
    "    v2.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "    v2.RandomEqualize(0.4),\n",
    "    v2.AugMix(),\n",
    "    v2.RandomHorizontalFlip(p=0.3),\n",
    "    v2.RandomVerticalFlip(0.3),\n",
    "    v2.GaussianBlur((3,3)),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.Resize(size=(50,50)),\n",
    "    v2.PILToTensor(),\n",
    "\n",
    "])\n",
    "validation_transforms =  v2.Compose([\n",
    "    v2.Resize(size=(50,50)),\n",
    "    v2.PILToTensor(),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:19:01.604797084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_set.dataset.transform = train_transforms\n",
    "validation_set.dataset.transform = validation_transforms\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:19:02.206542619Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_set,batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T15:19:02.209562835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:19:02.572374697Z",
     "start_time": "2023-11-29T15:19:02.473877766Z"
    },
    "id": "GQt7skizSYHo"
   },
   "outputs": [],
   "source": [
    "class GTSRB_MODEL(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(GTSRB_MODEL,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "      \n",
    "        self.metrics = {}\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "       \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3)\n",
    "        self.conv6 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "       \n",
    "       \n",
    "\n",
    "        self.l1 = nn.Linear(1024*4*4,512)\n",
    "        self.l2 = nn.Linear(512,128)\n",
    "        self.batchnorm4 = nn.LayerNorm(128)\n",
    "        self.l3 = nn.Linear(128,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        conv = self.conv1(input)\n",
    "        conv = self.conv2(conv)\n",
    "        batchnorm = self.relu(self.batchnorm1(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv3(maxpool)\n",
    "        conv = self.conv4(conv)\n",
    "        batchnorm = self.relu(self.batchnorm2(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv5(maxpool)\n",
    "        conv = self.conv6(conv)\n",
    "        batchnorm = self.relu(self.batchnorm3(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "        \n",
    "        \n",
    "        flatten = self.flatten(maxpool)\n",
    "        \n",
    "        dense_l1 = self.l1(flatten)\n",
    "        dropout = self.dropout3(dense_l1)\n",
    "        dense_l2 = self.l2(dropout)\n",
    "        batchnorm = self.batchnorm4(dense_l2)\n",
    "        dropout = self.dropout2(batchnorm)\n",
    "        output = self.l3(dropout)\n",
    "        \n",
    "       \n",
    "        return output\n",
    "    \n",
    "    def training_metrics(self,positives,data_size,loss):\n",
    "        acc = positives/data_size\n",
    "        return loss,acc\n",
    "    \n",
    "    def validation_metrics(self,validation_data,loss_function):\n",
    "       data_size = len(validation_data)\n",
    "       correct_predictions = 0\n",
    "       total_samples = 0\n",
    "       val_loss = 0\n",
    "\n",
    "       model = self.eval()\n",
    "       with torch.no_grad() : \n",
    "        for step,(input,label) in enumerate(validation_data):\n",
    "            input = input.to(torch.float)\n",
    "\n",
    "            input,label = input.to(device),label.to(device)\n",
    "            prediction = model.forward(input)\n",
    "            loss = loss_function(prediction,label)\n",
    "            val_loss = loss.item()\n",
    "            _,predicted = torch.max(prediction,1)\n",
    "            correct_predictions += (predicted == label).sum().item()\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "       val_acc = correct_predictions/total_samples\n",
    "\n",
    "       return val_loss,val_acc\n",
    "\n",
    "    def history(self):\n",
    "        return self.metrics\n",
    "\n",
    "    def compile(self,train_data,validation_data,epochs,loss_function,optimizer,learning_rate_scheduler):\n",
    "        val_acc_list = []\n",
    "        val_loss_list = []\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_loss_list = []\n",
    "\n",
    "        learning_rate_list = []\n",
    "\n",
    "        print('training started ...')\n",
    "        STEPS = len(train_data)\n",
    "        for epoch in range(epochs):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            learning_rate_list.append(lr)\n",
    "            correct_predictions = 0\n",
    "            total_examples = 0\n",
    "            loss = 0\n",
    "            with tqdm.trange(STEPS) as progress:\n",
    "\n",
    "                for step,(input,label) in enumerate(train_loader):\n",
    "                    input = input.to(torch.float)\n",
    "#                     input.type()\n",
    "\n",
    "\n",
    "                    input,label = input.to(device),label.to(device)\n",
    "                    prediction = self.forward(input)\n",
    "\n",
    "                    _, predicted = torch.max(prediction, 1)\n",
    "                    correct_predictions += (predicted == label).sum().item()\n",
    "                    total_examples += label.size(0)\n",
    "                    l = loss_function(prediction,label)\n",
    "                    loss = l.item()\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    progress.colour = 'green'\n",
    "                    progress.desc = f'Epoch [{epoch}/{EPOCHS}], Step [{step}/{STEPS}], Learning Rate [{lr}], Loss [{\"{:.4f}\".format(l)}], Accuracy [{\"{:.4f}\".format(correct_predictions/total_examples)}]'\n",
    "                    progress.update(1)\n",
    "\n",
    "            training_loss,training_acc = self.training_metrics(correct_predictions,total_examples,loss)\n",
    "            train_acc_list.append(training_acc)\n",
    "            train_loss_list.append(training_loss)\n",
    "\n",
    "            val_loss, val_acc = self.validation_metrics(validation_data,loss_function)\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            print(f'val_accuracy [{val_acc}], val_loss [{val_loss}]')\n",
    "\n",
    "            \n",
    "            learning_rate_scheduler.step()\n",
    "        \n",
    "        metrics_dict = {\n",
    "                'train_acc':train_acc_list,\n",
    "                'train_loss':train_loss_list,\n",
    "                'val_acc':val_acc_list,\n",
    "                'val_loss':val_loss_list,\n",
    "                'learning_rate':optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "        self.metrics = metrics_dict\n",
    "        print('training complete !')    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:19:02.818851184Z",
     "start_time": "2023-11-29T15:19:02.575830716Z"
    },
    "id": "sqCPI1ZhSxa5"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0008\n",
    "INPUT_DIM = 3*50*50\n",
    "OUTPUT_DIM = 43\n",
    "model = GTSRB_MODEL(INPUT_DIM,OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(),lr=LEARNING_RATE)\n",
    "lr_s = lr_scheduler.LinearLR(optimizer,start_factor=1.0,end_factor=0.5,total_iters=10)\n",
    "loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:29:46.053099564Z",
     "start_time": "2023-11-29T15:19:02.821583982Z"
    },
    "id": "X2U_RLsFVYCC",
    "outputId": "f36d0b32-8782-49f5-b33f-ddb37e86be51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Step [6/491], Learning Rate [0.0008], Loss [3.4349], Accuracy [0.0536]:   1%|\u001B[32m \u001B[0m| 7/491 [00:09<1\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m              \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m              \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m              \u001B[49m\u001B[43mlearning_rate_scheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr_s\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[35], line 138\u001B[0m, in \u001B[0;36mGTSRB_MODEL.compile\u001B[0;34m(self, train_data, validation_data, epochs, loss_function, optimizer, learning_rate_scheduler)\u001B[0m\n\u001B[1;32m    136\u001B[0m l \u001B[38;5;241m=\u001B[39m loss_function(prediction,label)\n\u001B[1;32m    137\u001B[0m loss \u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m--> 138\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    140\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(train_data=train_loader,\n",
    "              validation_data=validation_loader,\n",
    "              epochs=EPOCHS,\n",
    "              loss_function=loss,\n",
    "              optimizer=optimizer,\n",
    "              learning_rate_scheduler=lr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:39:03.739235828Z",
     "start_time": "2023-11-29T15:39:02.936845683Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-23T22:13:37.972041Z",
     "iopub.status.busy": "2023-11-23T22:13:37.971705Z",
     "iopub.status.idle": "2023-11-23T22:13:38.256394Z",
     "shell.execute_reply": "2023-11-23T22:13:38.255106Z",
     "shell.execute_reply.started": "2023-11-23T22:13:37.972011Z"
    },
    "id": "KBRfcAdsUSvA"
   },
   "outputs": [],
   "source": [
    "# saving a checkpoint assuming the network class named ClassNet\n",
    "checkpoint = {'model': model,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'epoch': EPOCHS,\n",
    "              'loss': loss,}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T02:30:24.224539741Z",
     "start_time": "2023-11-30T02:30:24.052014337Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-22T05:18:34.630378Z",
     "iopub.status.busy": "2023-11-22T05:18:34.629376Z",
     "iopub.status.idle": "2023-11-22T05:18:34.873541Z",
     "shell.execute_reply": "2023-11-22T05:18:34.872435Z",
     "shell.execute_reply.started": "2023-11-22T05:18:34.630323Z"
    },
    "id": "F7ahee-LU68o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Loading model\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "\n",
    "    model = checkpoint['model']  # 提取网络结构\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # 加载网络权重参数\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # 加载优化器参数\n",
    "    # epoch = checkpoint['epoch']\n",
    "    # loss = checkpoint['loss']\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "trained_model = load_checkpoint('/home/amber/Documents/sign/model/checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T02:17:40.593446872Z",
     "start_time": "2023-11-30T02:17:40.537780380Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.Resize(size=(50,50)),\n",
    "    v2.ToImageTensor(),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T02:24:00.017078908Z",
     "start_time": "2023-11-30T02:23:59.774086961Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f8cc418de40>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = GTSRB(root='/home/amber/Documents/sign/gtsrb-german-traffic-sign',split='test',transform=transforms)\n",
    "test_dataloader = DataLoader(dataset = test_dataset)\n",
    "print(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T02:24:24.751983148Z",
     "start_time": "2023-11-30T02:24:02.221832099Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-22T05:18:34.876593Z",
     "iopub.status.busy": "2023-11-22T05:18:34.875274Z",
     "iopub.status.idle": "2023-11-22T05:18:37.071973Z",
     "shell.execute_reply": "2023-11-22T05:18:37.068200Z",
     "shell.execute_reply.started": "2023-11-22T05:18:34.876547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 1.0 : 100%|\u001B[31m██████████\u001B[0m| 12630/12630 [00:22<00:00, 561.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "trained_model = trained_model.eval().to(device)\n",
    "with tqdm.tqdm(colour='red',total=len(test_dataloader)) as progress:\n",
    "  \n",
    "  with torch.no_grad() : \n",
    "    for id,(input,label) in enumerate(iter(test_dataloader)):\n",
    "        input = input.to(torch.float)\n",
    "        input,label = input.to(device),label\n",
    "        y_true = label.cpu().detach().numpy()\n",
    "        \n",
    "        prediction = trained_model.forward(input)\n",
    "        _,prediction = torch.max(prediction,1)\n",
    "        y_pred = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        progress.desc = f'Test Accuracy : {accuracy_score(y_true,y_pred)} '\n",
    "        progress.update(1)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-29T15:29:47.429941987Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 82373,
     "sourceId": 191501,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
